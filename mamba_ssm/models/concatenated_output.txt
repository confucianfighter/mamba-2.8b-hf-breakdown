
--------------------------------------------------
C:\Users\Daylan\Mamba2\mamba\mamba_ssm\models\config_mamba.py

--------------------------------------------------
1: from dataclasses import dataclass, field
2: 
3: 
4: @dataclass
5: class MambaConfig:
6: 
7:     d_model: int = 2560
8:     d_intermediate: int = 0
9:     n_layer: int = 64
10:     vocab_size: int = 50277
11:     ssm_cfg: dict = field(default_factory=dict)
12:     attn_layer_idx: list = field(default_factory=list)
13:     attn_cfg: dict = field(default_factory=dict)
14:     rms_norm: bool = True
15:     residual_in_fp32: bool = True
16:     fused_add_norm: bool = True
17:     pad_vocab_size_multiple: int = 8
18:     tie_embeddings: bool = True
19: 
--------------------------------------------------
C:\Users\Daylan\Mamba2\mamba\mamba_ssm\models\mixer_seq_simple.py

--------------------------------------------------
1: # Copyright (c) 2023, Albert Gu, Tri Dao.
2: 
3: import math
4: from functools import partial
5: import json
6: import os
7: import copy
8: 
9: from collections import namedtuple
10: 
11: import torch
12: import torch.nn as nn
13: 
14: from mamba_ssm.models.config_mamba import MambaConfig
15: from mamba_ssm.modules.mamba_simple import Mamba
16: from mamba_ssm.modules.mamba2 import Mamba2
17: from mamba_ssm.modules.mha import MHA
18: from mamba_ssm.modules.mlp import GatedMLP
19: from mamba_ssm.modules.block import Block
20: from mamba_ssm.utils.generation import GenerationMixin
21: from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf
22: 
23: try:
24:     from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn
25: except ImportError:
26:     RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None
27: 
28: 
29: def create_block(
30:     d_model,
31:     d_intermediate,
32:     ssm_cfg=None,
33:     attn_layer_idx=None,
34:     attn_cfg=None,
35:     norm_epsilon=1e-5,
36:     rms_norm=False,
37:     residual_in_fp32=False,
38:     fused_add_norm=False,
39:     layer_idx=None,
40:     device=None,
41:     dtype=None,
42: ):
43:     if ssm_cfg is None:
44:         ssm_cfg = {}
45:     if attn_layer_idx is None:
46:         attn_layer_idx = []
47:     if attn_cfg is None:
48:         attn_cfg = {}
49:     factory_kwargs = {"device": device, "dtype": dtype}
50:     if layer_idx not in attn_layer_idx:
51:         # Create a copy of the config to modify
52:         ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}
53:         ssm_layer = ssm_cfg.pop("layer", "Mamba1")
54:         if ssm_layer not in ["Mamba1", "Mamba2"]:
55:             raise ValueError(f"Invalid ssm_layer: {ssm_layer}, only support Mamba1 and Mamba2")
56:         mixer_cls = partial(
57:             Mamba2 if ssm_layer == "Mamba2" else Mamba,
58:             layer_idx=layer_idx,
59:             **ssm_cfg,
60:             **factory_kwargs
61:         )
62:     else:
63:         mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)
64:     norm_cls = partial(
65:         nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
66:     )
67:     if d_intermediate == 0:
68:         mlp_cls = nn.Identity
69:     else:
70:         mlp_cls = partial(
71:             GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs
72:         )
73:     block = Block(
74:         d_model,
75:         mixer_cls,
76:         mlp_cls,
77:         norm_cls=norm_cls,
78:         fused_add_norm=fused_add_norm,
79:         residual_in_fp32=residual_in_fp32,
80:     )
81:     block.layer_idx = layer_idx
82:     return block
83: 
84: 
85: # https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
86: def _init_weights(
87:     module,
88:     n_layer,
89:     initializer_range=0.02,  # Now only used for embedding layer.
90:     rescale_prenorm_residual=True,
91:     n_residuals_per_layer=1,  # Change to 2 if we have MLP
92: ):
93:     if isinstance(module, nn.Linear):
94:         if module.bias is not None:
95:             if not getattr(module.bias, "_no_reinit", False):
96:                 nn.init.zeros_(module.bias)
97:     elif isinstance(module, nn.Embedding):
98:         nn.init.normal_(module.weight, std=initializer_range)
99: 
100:     if rescale_prenorm_residual:
101:         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
102:         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
103:         #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.
104:         #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
105:         #
106:         # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
107:         for name, p in module.named_parameters():
108:             if name in ["out_proj.weight", "fc2.weight"]:
109:                 # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
110:                 # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
111:                 # We need to reinit p since this code could be called multiple times
112:                 # Having just p *= scale would repeatedly scale it down
113:                 nn.init.kaiming_uniform_(p, a=math.sqrt(5))
114:                 with torch.no_grad():
115:                     p /= math.sqrt(n_residuals_per_layer * n_layer)
116: 
117: 
118: class MixerModel(nn.Module):
119:     def __init__(
120:         self,
121:         d_model: int,
122:         n_layer: int,
123:         d_intermediate: int,
124:         vocab_size: int,
125:         ssm_cfg=None,
126:         attn_layer_idx=None,
127:         attn_cfg=None,
128:         norm_epsilon: float = 1e-5,
129:         rms_norm: bool = False,
130:         initializer_cfg=None,
131:         fused_add_norm=False,
132:         residual_in_fp32=False,
133:         device=None,
134:         dtype=None,
135:     ) -> None:
136:         factory_kwargs = {"device": device, "dtype": dtype}
137:         super().__init__()
138:         self.residual_in_fp32 = residual_in_fp32
139: 
140:         self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)
141: 
142:         # We change the order of residual and layer norm:
143:         # Instead of LN -> Attn / MLP -> Add, we do:
144:         # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and
145:         # the main branch (output of MLP / Mixer). The model definition is unchanged.
146:         # This is for performance reason: we can fuse add + layer_norm.
147:         self.fused_add_norm = fused_add_norm
148:         if self.fused_add_norm:
149:             if layer_norm_fn is None or rms_norm_fn is None:
150:                 raise ImportError("Failed to import Triton LayerNorm / RMSNorm kernels")
151: 
152:         self.layers = nn.ModuleList(
153:             [
154:                 create_block(
155:                     d_model,
156:                     d_intermediate=d_intermediate,
157:                     ssm_cfg=ssm_cfg,
158:                     attn_layer_idx=attn_layer_idx,
159:                     attn_cfg=attn_cfg,
160:                     norm_epsilon=norm_epsilon,
161:                     rms_norm=rms_norm,
162:                     residual_in_fp32=residual_in_fp32,
163:                     fused_add_norm=fused_add_norm,
164:                     layer_idx=i,
165:                     **factory_kwargs,
166:                 )
167:                 for i in range(n_layer)
168:             ]
169:         )
170: 
171:         self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
172:             d_model, eps=norm_epsilon, **factory_kwargs
173:         )
174: 
175:         self.apply(
176:             partial(
177:                 _init_weights,
178:                 n_layer=n_layer,
179:                 **(initializer_cfg if initializer_cfg is not None else {}),
180:                 n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP
181:             )
182:         )
183: 
184:     def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
185:         return {
186:             i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
187:             for i, layer in enumerate(self.layers)
188:         }
189: 
190:     def forward(self, input_ids, inference_params=None, **mixer_kwargs):
191:         hidden_states = self.embedding(input_ids)
192:         residual = None
193:         for layer in self.layers:
194:             hidden_states, residual = layer(
195:                 hidden_states, residual, inference_params=inference_params
196:             )
197:         if not self.fused_add_norm:
198:             residual = (hidden_states + residual) if residual is not None else hidden_states
199:             hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
200:         else:
201:             # Set prenorm=False here since we don't need the residual
202:             hidden_states = layer_norm_fn(
203:                 hidden_states,
204:                 self.norm_f.weight,
205:                 self.norm_f.bias,
206:                 eps=self.norm_f.eps,
207:                 residual=residual,
208:                 prenorm=False,
209:                 residual_in_fp32=self.residual_in_fp32,
210:                 is_rms_norm=isinstance(self.norm_f, RMSNorm)
211:             )
212:         return hidden_states
213: 
214: 
215: class MambaLMHeadModel(nn.Module, GenerationMixin):
216: 
217:     def __init__(
218:         self,
219:         config: MambaConfig,
220:         initializer_cfg=None,
221:         device=None,
222:         dtype=None,
223:     ) -> None:
224:         self.config = config
225:         d_model = config.d_model
226:         n_layer = config.n_layer
227:         d_intermediate = config.d_intermediate
228:         vocab_size = config.vocab_size
229:         ssm_cfg = config.ssm_cfg
230:         attn_layer_idx = config.attn_layer_idx
231:         attn_cfg = config.attn_cfg
232:         rms_norm = config.rms_norm
233:         residual_in_fp32 = config.residual_in_fp32
234:         fused_add_norm = config.fused_add_norm
235:         pad_vocab_size_multiple = config.pad_vocab_size_multiple
236:         factory_kwargs = {"device": device, "dtype": dtype}
237: 
238:         super().__init__()
239:         if vocab_size % pad_vocab_size_multiple != 0:
240:             vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)
241:         self.backbone = MixerModel(
242:             d_model=d_model,
243:             n_layer=n_layer,
244:             d_intermediate=d_intermediate,
245:             vocab_size=vocab_size,
246:             ssm_cfg=ssm_cfg,
247:             attn_layer_idx=attn_layer_idx,
248:             attn_cfg=attn_cfg,
249:             rms_norm=rms_norm,
250:             initializer_cfg=initializer_cfg,
251:             fused_add_norm=fused_add_norm,
252:             residual_in_fp32=residual_in_fp32,
253:             **factory_kwargs,
254:         )
255:         self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)
256: 
257:         # Initialize weights and apply final processing
258:         self.apply(
259:             partial(
260:                 _init_weights,
261:                 n_layer=n_layer,
262:                 **(initializer_cfg if initializer_cfg is not None else {}),
263:             )
264:         )
265:         self.tie_weights()
266: 
267:     def tie_weights(self):
268:         if self.config.tie_embeddings:
269:             self.lm_head.weight = self.backbone.embedding.weight
270: 
271:     def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
272:         return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
273: 
274:     def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, **mixer_kwargs):
275:         """
276:         "position_ids" is just to be compatible with Transformer generation. We don't use it.
277:         num_last_tokens: if > 0, only return the logits for the last n tokens
278:         """
279:         hidden_states = self.backbone(input_ids, inference_params=inference_params, **mixer_kwargs)
280:         if num_last_tokens > 0:
281:             hidden_states = hidden_states[:, -num_last_tokens:]
282:         lm_logits = self.lm_head(hidden_states)
283:         CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
284:         return CausalLMOutput(logits=lm_logits)
285: 
286:     @classmethod
287:     def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):
288:         config_data = load_config_hf(pretrained_model_name)
289:         config = MambaConfig(**config_data)
290:         model = cls(config, device=device, dtype=dtype, **kwargs)
291:         model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
292:         return model
293: 
294:     def save_pretrained(self, save_directory):
295:         """
296:         Minimal implementation of save_pretrained for MambaLMHeadModel.
297:         Save the model and its configuration file to a directory.
298:         """
299:         # Ensure save_directory exists
300:         os.makedirs(save_directory, exist_ok=True)
301: 
302:         # Save the model's state_dict
303:         model_path = os.path.join(save_directory, 'pytorch_model.bin')
304:         torch.save(self.state_dict(), model_path)
305: 
306:         # Save the configuration of the model
307:         config_path = os.path.join(save_directory, 'config.json')
308:         with open(config_path, 'w') as f:
309:             json.dump(self.config.__dict__, f, indent=4)
310: 
--------------------------------------------------
C:\Users\Daylan\Mamba2\mamba\mamba_ssm\models\__init__.py

--------------------------------------------------
1: 